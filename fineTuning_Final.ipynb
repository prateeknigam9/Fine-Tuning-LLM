{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U accelerate git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q datasets einops wandb\n",
    "# !pip install -q bitsandbytes==0.43.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:54.9592395Z\",\"response\":\"Hello\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:55.2628976Z\",\"response\":\"!\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:55.5477783Z\",\"response\":\" Is\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:55.7775636Z\",\"response\":\" there\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:55.9435867Z\",\"response\":\" something\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:56.1088784Z\",\"response\":\" I\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:56.2687449Z\",\"response\":\" can\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:56.4245836Z\",\"response\":\" help\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:56.7512666Z\",\"response\":\" you\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:56.9115365Z\",\"response\":\" with\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:57.1037695Z\",\"response\":\" or\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:57.3118544Z\",\"response\":\" would\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:57.653285Z\",\"response\":\" you\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:57.8111748Z\",\"response\":\" like\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:58.0860132Z\",\"response\":\" to\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:58.4058095Z\",\"response\":\" chat\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:58.7054684Z\",\"response\":\"?\",\"done\":false}\\n{\"model\":\"llama3.2-vision\",\"created_at\":\"2025-02-19T19:55:58.861534Z\",\"response\":\"\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,882,128007,271,6151,128009,128006,78191,128007,271,9906,0,2209,1070,2555,358,649,1520,499,449,477,1053,499,1093,311,6369,30],\"total_duration\":4692423900,\"load_duration\":284954300,\"prompt_eval_count\":11,\"prompt_eval_duration\":500000000,\"eval_count\":18,\"eval_duration\":3902000000}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://90ca-140-203-228-110.ngrok-free.app/api/generate\"\n",
    "\n",
    "data = {\n",
    "    'model': 'llama3.2-vision',\n",
    "    'prompt':\"hi\"\n",
    "}\n",
    "\n",
    "response = requests.post(url,json = data)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112165"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Malikeh1375/medical-question-answering-datasets\", \"chatdoctor_healthcaremagic\",split=\"train\")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"deepseek-r1:1.5b\"\n",
    "# model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "url = \"https://90ca-140-203-2123-1230.ngrok-free.app/api/generate\"\n",
    "model_name= url + \"deepseek-r1:1.5b\"\n",
    "\n",
    "# mod_nm = OllamaLLM(model=model_name)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "url = \"https://90ca-140-203-2123-1230.ngrok-free.app/api/generate\"\n",
    "model_name= url + \"deepseek-r1:1.5b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             quantization_config = nf4_config,\n",
    "                                             device_map = \"auto\",\n",
    "                                             trust_remote_code = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaLLM(model='deepseek-r1:1.5b')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "olama_model = OllamaLLM(model='deepseek-r1:1.5b')\n",
    "olama_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 16\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_config = LoraConfig(inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1, peft_type=TaskType.CAUSAL_LM)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(data_point):\n",
    "    prompt = f\"\"\"###SYSTEM: {data_point['instruction']}\n",
    "\n",
    "    ###INPUT: {data_point['input']}\n",
    "\n",
    "    ###Response: {data_point['output']}\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(prompt,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",)\n",
    "    tokens[\"labels\"] = tokens['input_ids'].copy()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(format_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns(['input', \"output\",\"instruction\"])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.device_count() > 1: \n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./lora_finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"./lora_finetuned_model\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, \"./lora_finetuned_model\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.. Thank you doc!\"\n",
    "prompt = f\"\"\"\n",
    "### 'instruction': \"If you are a doctor, please answer the medical questions based on the patient's description.\"\n",
    "\n",
    "### input : {input_query}\n",
    "\n",
    "### output : \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids, num_return_sequences=1)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
